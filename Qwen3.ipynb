{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73f39f76",
   "metadata": {},
   "source": [
    "## How to Download and Run Qwen3 \n",
    "### What we’ll demo (locally, CPU)\n",
    "\n",
    "- Text embeddings with Qwen/Qwen3-Embedding-0.6B (best choice for CPU).\n",
    "- Similarity search (cosine) over a small doc set.\n",
    "- Instruction-aware embeddings (query/task prefix).\n",
    "- Multilingual example (English/Hindi).\n",
    "- Reranking with a CPU-friendly Qwen3 reranker variant (sequence-classification head).\n",
    "\n",
    "Why this model: Qwen3-Embedding (0.6B/4B/8B) is purpose-built for retrieval & reranking, trained with a multi-stage pipeline and supports instruction-aware inputs; it’s a new (June 5 2025) series that’s SoTA on many MTEB-style retrieval tasks. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e31e7c",
   "metadata": {},
   "source": [
    "### Set up Environment (CPU Only)\n",
    "#### Fresh venv (Windows PowerShell shown; macOS/Linux: replace python with python3)\n",
    "- python -m venv .venv\n",
    "- . .venv/Scripts/Activate.ps1    # Windows\n",
    "- pip install --upgrade pip\n",
    "- pip install torch --index-url https://download.pytorch.org/whl/cpu\n",
    "- pip install transformers>=4.44.0 accelerate sentencepiece scipy numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc565a6",
   "metadata": {},
   "source": [
    "### Features to be presented \n",
    "\n",
    "- Instruction-aware embeddings: show how adding Instruct: ... improves retrieval. (Paper/blog describe instruction-aware inputs.) \n",
    "- Multilingual retrieval: run a Hindi (or your choice) query—still retrieves the English “New Delhi” doc. (Multilingual capability is a core selling point.) \n",
    "- Lightweight CPU footprint: 0.6B runs on CPU for quick demos (your scripts above).\n",
    "- Reranking stage: show base retrieval (cosine top-k) vs reranked order—the relevant doc pops to #1. The paper defines the yes/no likelihood method; our seq-cls variant emulates it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e042561",
   "metadata": {},
   "source": [
    "### Quick embedding demo (script) using \"AutoModel\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b3ab050",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PuttarajuS\\Desktop\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: What is the capital of India?\n",
      "  1. score=0.807 -> New Delhi is the capital of India.\n",
      "  2. score=0.557 -> Mumbai is India’s financial center.\n",
      "  3. score=0.479 -> Washington, D.C. is the capital of the United States.\n",
      "\n",
      "Query: भारत की राजधानी क्या है?\n",
      "  1. score=0.725 -> New Delhi is the capital of India.\n",
      "  2. score=0.552 -> Mumbai is India’s financial center.\n",
      "  3. score=0.428 -> Washington, D.C. is the capital of the United States.\n"
     ]
    }
   ],
   "source": [
    "import torch, torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "MODEL_ID = \"Qwen/Qwen3-Embedding-0.6B\"   # CPU-friendly\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_ID, padding_side=\"left\")\n",
    "model = AutoModel.from_pretrained(MODEL_ID)  # defaults to CPU\n",
    "\n",
    "def last_token_pool(last_hidden_states, attention_mask):\n",
    "    # Qwen3-Embedding uses last-token pooling (see README/blog)\n",
    "    left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n",
    "    if left_padding:\n",
    "        return last_hidden_states[:, -1]\n",
    "    else:\n",
    "        seq_lengths = attention_mask.sum(dim=1) - 1\n",
    "        bsz = last_hidden_states.shape[0]\n",
    "        return last_hidden_states[torch.arange(bsz), seq_lengths]\n",
    "\n",
    "def embed_texts(texts, instruction=None):\n",
    "    # Instruction-aware format recommended by authors\n",
    "    prefix = f\"Instruct: {instruction}\\nQuery: \" if instruction else \"\"\n",
    "    # Qwen3-Embedding expects the final token to be the end token\n",
    "    texts = [f\"{prefix}{t}{tok.eos_token}\" for t in texts]\n",
    "    enc = tok(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        out = model(**enc)\n",
    "        pooled = last_token_pool(out.last_hidden_state, enc[\"attention_mask\"])\n",
    "        # L2 normalize (recommended)\n",
    "        emb = F.normalize(pooled, p=2, dim=1)\n",
    "    return emb\n",
    "\n",
    "def cos_sim(a, b):\n",
    "    return (a @ b.T).cpu().numpy()\n",
    "\n",
    "# --- Demo data ---\n",
    "instruction = \"Given a web search query, retrieve relevant passages that answer the query\"\n",
    "queries = [\n",
    "    \"What is the capital of India?\",\n",
    "    \"भारत की राजधानी क्या है?\",  # Hindi\n",
    "]\n",
    "docs = [\n",
    "    \"New Delhi is the capital of India.\",\n",
    "    \"Washington, D.C. is the capital of the United States.\",\n",
    "    \"Mumbai is India’s financial center.\",\n",
    "]\n",
    "\n",
    "# Compute embeddings\n",
    "q_emb = embed_texts(queries, instruction=instruction)\n",
    "d_emb = embed_texts(docs, instruction=instruction)\n",
    "\n",
    "# Similarity search\n",
    "S = cos_sim(q_emb, d_emb)\n",
    "for i,q in enumerate(queries):\n",
    "    order = S[i].argsort()[::-1]\n",
    "    print(f\"\\nQuery: {q}\")\n",
    "    for j,idx in enumerate(order):\n",
    "        print(f\"  {j+1}. score={S[i,idx]:.3f} -> {docs[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d081965",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PuttarajuS\\Desktop\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Name the administrative capital city of the Republic of India.\n",
      "0.637  ->  Delhi is a historic city with many monuments.\n",
      "0.506  ->  The Indian rupee is the currency of India.\n",
      "0.453  ->  Washington D C is known for historical Museums.\n"
     ]
    }
   ],
   "source": [
    "import torch, math\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "mid = \"Qwen/Qwen3-Embedding-0.6B\"\n",
    "tok = AutoTokenizer.from_pretrained(mid)\n",
    "model = AutoModel.from_pretrained(mid)\n",
    "\n",
    "def embed_text(texts):\n",
    "    enc = tok(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        out = model(**enc)\n",
    "    # Per model card: pool on last token (EOS is auto-added in latest tokenizer)\n",
    "    last_hidden = out.last_hidden_state\n",
    "    # compute positions of last non-pad token in each sequence\n",
    "    seq_lens = enc[\"attention_mask\"].sum(dim=1) - 1\n",
    "    pooled = last_hidden[torch.arange(last_hidden.size(0)), seq_lens]\n",
    "    # L2-normalize for cosine similarity\n",
    "    return torch.nn.functional.normalize(pooled, p=2, dim=1)\n",
    "\"\"\"\n",
    "queries = [\"What is the capital of India?\"]\n",
    "docs    = [\"New Delhi is the capital of India.\",\n",
    "           \"The capital of United States of America is Washington D C.\",\n",
    "           \"Bombay is the Financial district in India.\"]\n",
    "\"\"\"\n",
    "queries = [\"Name the administrative capital city of the Republic of India.\"]\n",
    "docs    = [\"Washington D C is known for historical Museums.\",\n",
    "           \"Delhi is a historic city with many monuments.\",\n",
    "           \"The Indian rupee is the currency of India.\"]\n",
    "\n",
    "\n",
    "E_q = embed_text(queries)\n",
    "E_d = embed_text(docs)\n",
    "\n",
    "# cosine similarity q vs each doc\n",
    "sims = (E_q @ E_d.T).squeeze(0)\n",
    "order = torch.argsort(sims, descending=True).tolist()\n",
    "\n",
    "print(\"Query:\", queries[0])\n",
    "for i in order:\n",
    "    print(f\"{sims[i].item():.3f}  ->  {docs[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45018cb3",
   "metadata": {},
   "source": [
    "### Qwen3 Reranking demo (script) Seq2Seq using \"AutoModelForSequenceClassification\"  \n",
    "\n",
    "The original Qwen3-Reranker computes a yes/no log-odds score on (instruction, query, document). For a CPU-simple path, we’ll use a sequence-classification conversion of the 0.6B reranker—works nicely with standard AutoModelForSequenceClassification and returns a single relevance logit. (This conversion approach is documented in community repos & vLLM notes.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec897968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rerank documents for relevance to the query.\n",
      "['Delhi is a historic city with many monuments.', \"India's seat of government is located in New Delhi.\", 'The Indian rupee is the currency of India.', 'Washington D. C. is known for historical museums.']\n",
      "Name the administrative capital city of the Republic of India.\n",
      "Reranked:\n",
      "  1. score=1.398 -> Delhi is a historic city with many monuments.\n",
      "  2. score=1.354 -> India's seat of government is located in New Delhi.\n",
      "  3. score=-0.437 -> Washington D. C. is known for historical museums.\n",
      "  4. score=-0.572 -> The Indian rupee is the currency of India.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "MODEL_ID = \"tomaarsen/Qwen3-Reranker-0.6B-seq-cls\"  # CPU-friendly variant\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_ID)  # CPU by default\n",
    "model.eval()\n",
    "\n",
    "instruction = \"Rerank documents for relevance to the query.\"\n",
    "query = \"Name the administrative capital city of the Republic of India.\"\n",
    "docs = [\n",
    "    \"Delhi is a historic city with many monuments.\",\n",
    "    \"India's seat of government is located in New Delhi.\",\n",
    "    \"The Indian rupee is the currency of India.\",\n",
    "    \"Washington D. C. is known for historical museums.\",\n",
    "]\n",
    "\n",
    "pairs = [(\n",
    "    f\"Instruct: {instruction}\\nQuery: {query}\",\n",
    "    d\n",
    ") for d in docs]\n",
    "\n",
    "enc = tok([q for q,d in pairs],\n",
    "          [d for q,d in pairs],\n",
    "          padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**enc).logits.squeeze(-1)  # higher = more relevant\n",
    "\n",
    "order = torch.argsort(logits, descending=True).tolist()\n",
    "print(instruction)\n",
    "print(docs)\n",
    "print(query)\n",
    "print(\"Reranked:\")\n",
    "for rank, idx in enumerate(order, 1):\n",
    "    print(f\"  {rank}. score={float(logits[idx]):.3f} -> {docs[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43cd729d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PuttarajuS\\Desktop\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Name the administrative capital city of the Republic of India.\n",
      "2.872  ->  Indias seat of government is located in New Delhi.\n",
      "1.265  ->  The Indian rupee is the currency of India.\n",
      "0.965  ->  Washington D C is known for historical Museums.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "mid = \"tomaarsen/Qwen3-Reranker-0.6B-seq-cls\"  # converted to seq-cls\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(mid)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token or tok.sep_token or tok.cls_token\n",
    "tok.padding_side = \"left\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(mid)\n",
    "model.config.pad_token_id = tok.pad_token_id\n",
    "\"\"\"\n",
    "query = \"Best city to visit in India?\"\n",
    "docs  = [\n",
    "    \"New Delhi is the capital with historical sites.\",\n",
    "    \"Washington D C is known for historical Museums.\",\n",
    "    \"Bombay is a modern financial hub.\"\n",
    "]\n",
    "\"\"\"\n",
    "query = \"Name the administrative capital city of the Republic of India.\"\n",
    "docs  = [\n",
    "    \"Indias seat of government is located in New Delhi.\",\n",
    "    \"Washington D C is known for historical Museums.\",\n",
    "    \"The Indian rupee is the currency of India.\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# batch encode pairs\n",
    "enc = tok([query]*len(docs), docs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**enc).logits.squeeze(-1)  # shape [batch]\n",
    "\n",
    "order = torch.argsort(logits, descending=True).tolist()\n",
    "print(\"Query:\", query)\n",
    "for i in order:\n",
    "    print(f\"{logits[i].item():.3f}  ->  {docs[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d370d44",
   "metadata": {},
   "source": [
    "### Qwen3 Reranking with original yes/no format using \"AutoModelForCausalLM\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d59b0ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Name the administrative capital city of the Republic of India.\n",
      "2.214  ->  Washington D C is known for historical Museums.\n",
      "0.535  ->  Indias seat of government is located in New Delhi.\n",
      "-9.568  ->  Delhi is a historic city with many monuments.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "mid = \"Qwen/Qwen3-Reranker-0.6B\"  # official\n",
    "tok = AutoTokenizer.from_pretrained(mid)\n",
    "model = AutoModelForCausalLM.from_pretrained(mid)\n",
    "\n",
    "# Ensure padding works for batching\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token or tok.sep_token or tok.cls_token\n",
    "tok.padding_side = \"left\"\n",
    "model.config.pad_token_id = tok.pad_token_id\n",
    "\n",
    "# Minimal prompt template (matches common reranker usage)\n",
    "def build_prompt(query, doc):\n",
    "    # You can refine template per model card; core idea: ask yes/no relevance.\n",
    "    return f\"Query: {query}\\nDocument: {doc}\\nIs the document relevant to the query? Answer yes or no:\"\n",
    "\n",
    "def yes_no_scores(prompts):\n",
    "    enc = tok(prompts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        out = model(**enc)\n",
    "    # Take next-token logits at the last non-pad position\n",
    "    last_pos = enc[\"attention_mask\"].sum(dim=1) - 1\n",
    "    logits_next = out.logits[torch.arange(out.logits.size(0)), last_pos]  # [batch, vocab]\n",
    "    id_yes = tok.convert_tokens_to_ids(\" yes\")\n",
    "    id_no  = tok.convert_tokens_to_ids(\" no\")\n",
    "    # Fallback if leading space tokens aren't in vocab:\n",
    "    if id_yes is None: id_yes = tok.convert_tokens_to_ids(\"yes\")\n",
    "    if id_no  is None: id_no  = tok.convert_tokens_to_ids(\"no\")\n",
    "    # Score = logit_yes - logit_no (or softmax prob_yes)\n",
    "    yes = logits_next[:, id_yes]\n",
    "    no  = logits_next[:, id_no]\n",
    "    return (yes - no)\n",
    "\"\"\"\n",
    "query = \"Best city to visit in India?\"\n",
    "docs  = [\n",
    "    \"New Delhi is the capital with historical sites.\",\n",
    "    \"Washington D C is known for historical Museums.\",\n",
    "    \"Bombay is a modern financial hub.\"\n",
    "]\n",
    "\"\"\"\n",
    "query = \"Name the administrative capital city of the Republic of India.\"\n",
    "docs  = [\n",
    "    \"Indias seat of government is located in New Delhi.\",\n",
    "    \"Delhi is a historic city with many monuments.\",\n",
    "    \"Washington D C is known for historical Museums.\"\n",
    "]\n",
    "\n",
    "prompts = [build_prompt(query, d) for d in docs]\n",
    "scores = yes_no_scores(prompts)\n",
    "\n",
    "order = torch.argsort(scores, descending=True).tolist()\n",
    "print(\"Query:\", query)\n",
    "for i in order:\n",
    "    print(f\"{scores[i].item():.3f}  ->  {docs[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f198c9",
   "metadata": {},
   "source": [
    "### Original yes/no scoring — Hugging Face (CPU)\n",
    "#### What it does.\n",
    "For each (query, document) pair, it builds a yes/no prompt and computes the conditional log-probability of the continuations \" yes\" and \" no\" (tokenized) given the prompt. It returns a score = logp(yes) − logp(no). Higher = more relevant.\n",
    "\n",
    "Pick the smallest Qwen3 Instruct model you can run on CPU (it will still be slow). Example placeholders below; replace MODEL_ID with the smallest Qwen3 Instruct you have locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc60fa5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Name the administrative capital city of the Republic of India.\n",
      "\n",
      " 1. score=0.000  (logp_yes=0.000, logp_no=0.000)  -> Delhi is a historic city with many monuments.\n",
      " 2. score=0.000  (logp_yes=0.000, logp_no=0.000)  -> India's seat of government is located in New Delhi.\n",
      " 3. score=0.000  (logp_yes=0.000, logp_no=0.000)  -> The Indian rupee is the currency of India.\n",
      " 4. score=0.000  (logp_yes=0.000, logp_no=0.000)  -> Washington D. C. is known for historical museums.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# --- Choose a (small) Qwen3 Instruct model you can run locally on CPU ---\n",
    "# Example placeholder (replace with your local model):\n",
    "MODEL_ID = \"Qwen/Qwen2-1.5B-Instruct\"  # <-- replace with a Qwen3 Instruct if available locally\n",
    "DEVICE = \"cpu\"\n",
    "DTYPE = torch.float32\n",
    "\n",
    "# Prompt template (concise & deterministic)\n",
    "TEMPLATE = (\n",
    "    \"You are a reranker. Answer strictly with 'yes' or 'no'.\\n\"\n",
    "    \"Query: {query}\\n\"\n",
    "    \"Document: {doc}\\n\"\n",
    "    \"Answer:\"\n",
    ")\n",
    "\n",
    "def load_model(model_id=MODEL_ID):\n",
    "    tok = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "    if tok.eos_token is None:\n",
    "        tok.eos_token = tok.pad_token or \"<|endoftext|>\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=DTYPE)\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "    return tok, model\n",
    "\n",
    "def candidate_logprob(tok, model, prompt:str, candidate:str, max_new_tokens=None):\n",
    "    \"\"\"\n",
    "    Computes log p(candidate | prompt) by teacher-forcing the candidate tokens.\n",
    "    \"\"\"\n",
    "    # Encode prompt and prompt+candidate\n",
    "    enc_prompt = tok(prompt, return_tensors=\"pt\")\n",
    "    enc_full   = tok(prompt + candidate, return_tensors=\"pt\")\n",
    "\n",
    "    input_ids = enc_full.input_ids.to(DEVICE)\n",
    "    attn_mask = enc_full.attention_mask.to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = model(input_ids=input_ids, attention_mask=attn_mask)\n",
    "        logits = out.logits  # [B, T, V]\n",
    "        # We need log-probs of candidate tokens only (positions after the prompt)\n",
    "        # Shift so that logits[t-1] -> token[t]\n",
    "        logprobs = torch.log_softmax(logits[:, :-1, :], dim=-1)  # [B, T-1, V]\n",
    "\n",
    "    # Indices that correspond to candidate tokens\n",
    "    prompt_len = tok(prompt, return_tensors=\"pt\").input_ids.shape[1]\n",
    "    full_len   = input_ids.shape[1]\n",
    "    cand_token_ids = input_ids[:, prompt_len:full_len]  # [B, Lc]\n",
    "\n",
    "    # Gather logprobs at candidate token positions\n",
    "    # Align: use the last T-1 vs tokens[1:]\n",
    "    relevant_logprobs = logprobs[:, prompt_len-1:full_len-1, :].gather(\n",
    "        dim=-1, index=cand_token_ids.unsqueeze(-1)\n",
    "    ).squeeze(-1)  # [B, Lc]\n",
    "\n",
    "    total_logprob = relevant_logprobs.sum(dim=1).item()\n",
    "    return float(total_logprob)\n",
    "\n",
    "def yes_no_score(tok, model, query:str, doc:str):\n",
    "    prompt = TEMPLATE.format(query=query, doc=doc).strip() + \" \"\n",
    "    # Try candidate variants with/without leading spaces to match tokenizer behavior\n",
    "    candidates = [\"yes\", \" yes\", \"Yes\", \" Yes\"]\n",
    "    cand_no    = [\"no\", \" no\", \"No\", \" No\"]\n",
    "\n",
    "    def best_logp(alts):\n",
    "        best = -1e30\n",
    "        for a in alts:\n",
    "            try:\n",
    "                lp = candidate_logprob(tok, model, prompt, a)\n",
    "                if lp > best:\n",
    "                    best = lp\n",
    "            except Exception:\n",
    "                pass\n",
    "        return best\n",
    "\n",
    "    lp_yes = best_logp(candidates)\n",
    "    lp_no  = best_logp(cand_no)\n",
    "    return lp_yes - lp_no, lp_yes, lp_no, prompt\n",
    "\n",
    "def rerank(tok, model, query, docs):\n",
    "    scored = []\n",
    "    for d in docs:\n",
    "        s, lp_y, lp_n, prompt = yes_no_score(tok, model, query, d)\n",
    "        scored.append((s, lp_y, lp_n, d))\n",
    "    scored.sort(key=lambda x: x[0], reverse=True)\n",
    "    return scored\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tok, model = load_model()\n",
    "\n",
    "    query = \"Name the administrative capital city of the Republic of India.\"\n",
    "    docs = [\n",
    "        \"Delhi is a historic city with many monuments.\",\n",
    "        \"India's seat of government is located in New Delhi.\",\n",
    "        \"The Indian rupee is the currency of India.\",\n",
    "        \"Washington D. C. is known for historical museums.\",\n",
    "    ]\n",
    "\n",
    "    results = rerank(tok, model, query, docs)\n",
    "    print(f\"Query: {query}\\n\")\n",
    "    for i,(score, lp_y, lp_n, d) in enumerate(results, 1):\n",
    "        print(f\"{i:2d}. score={score:.3f}  (logp_yes={lp_y:.3f}, logp_no={lp_n:.3f})  -> {d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191a7438",
   "metadata": {},
   "source": [
    "#### FAQ & tweaks\n",
    "Why check variants like \" yes\" vs \"Yes\"?\n",
    "\n",
    "Tokenizers differ in whether the leading space is part of the token. Trying a small set of variants makes the scorer robust.\n",
    "\n",
    "Can I batch with vLLM?\n",
    "\n",
    "Yes—send multiple prompts (or use an array of messages) and process responses; just keep max_tokens=1 and temperature=0.\n",
    "\n",
    "Why not generate the full “yes”/“no” string?\n",
    "\n",
    "One token is faster and sufficient. If your model tends to emit punctuation, keep variants like \" yes.\".\n",
    "\n",
    "How to integrate with your earlier embedding step?\n",
    "\n",
    "Use embeddings to get top-k candidates, then apply yes/no scoring to those k docs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
